name: Benchmark

on:
  push:
    branches: [main]
    paths:
      - 'Abies/**'
      - 'Abies.Benchmarks/**'
      - 'scripts/extract-allocations.py'
      - 'scripts/merge-benchmark-results.py'
      - 'scripts/compare-benchmarks.py'
      - 'scripts/compare-benchmark.py'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main]
    # No path filter for PRs - let the job-level conditions decide
    # E2E runs on perf PRs, micro runs on Abies/** changes
  workflow_dispatch:
    inputs:
      run_e2e_benchmark:
        description: 'Run E2E js-framework-benchmark'
        required: false
        default: true
        type: boolean

permissions:
  contents: write
  deployments: write
  pull-requests: write

# ==============================================================================
# BENCHMARKING STRATEGY (see docs/investigations/benchmarking-strategy.md)
# ==============================================================================
# This workflow uses js-framework-benchmark as the SINGLE SOURCE OF TRUTH.
#
# Why E2E only (no micro-benchmarks in PR builds):
# - Historical evidence: PatchType enum optimization showed 11-20% improvement
#   in BenchmarkDotNet but caused 2-5% REGRESSION in E2E benchmarks
# - Micro-benchmarks miss: JS interop overhead, browser rendering, GC pressure
# - E2E measures real user-perceived latency (EventDispatch → Paint)
#
# BenchmarkDotNet micro-benchmarks:
# - ONLY run on push to main for historical tracking
# - NOT run on PRs (removed to avoid misleading results)
# - For local development: dotnet run --project Abies.Benchmarks -c Release
# - Good for algorithm comparison and allocation tracking locally
#
# E2E benchmarks auto-trigger on:
# - PRs with title starting with 'perf:' or 'perf('
# - PRs with 'performance' label
# - PRs that modify benchmark workflow or scripts
# - Pushes to main (baseline tracking)
# - Manual workflow_dispatch
#
# Benchmarks included:
# - CPU (01-09): Create, replace, update, select, swap, remove, clear rows
# - Memory (21-25): Ready memory, run memory, clear memory
# ==============================================================================

jobs:
  # ============================================================================
  # PATH DETECTION - Determine which jobs should run
  # ============================================================================
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      e2e-scripts: ${{ steps.filter.outputs.e2e-scripts }}
      perf-pr: ${{ steps.check-perf.outputs.is-perf }}
    steps:
      - uses: actions/checkout@v6
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            e2e-scripts:
              - 'Abies/**'
              - '.github/workflows/benchmark.yml'
              - 'scripts/compare-benchmark.py'
              - 'scripts/convert-e2e-results.py'
              - 'scripts/run-benchmarks.sh'
              - 'contrib/js-framework-benchmark/**'
      - name: Check if performance PR
        id: check-perf
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            TITLE="${{ github.event.pull_request.title }}"
            if [[ "$TITLE" == perf:* ]] || [[ "$TITLE" == perf\(* ]]; then
              echo "is-perf=true" >> $GITHUB_OUTPUT
            else
              echo "is-perf=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "is-perf=false" >> $GITHUB_OUTPUT
          fi

  # ============================================================================
  # MICRO-BENCHMARKS (BenchmarkDotNet) - Historical Tracking Only
  # ============================================================================
  # ONLY runs on push to main for historical tracking.
  # NOT run on PRs - micro-benchmarks have historically been misleading.
  # For local development: dotnet run --project Abies.Benchmarks -c Release
  # ============================================================================
  benchmark:
    name: Micro-Benchmarks (main only)
    runs-on: ubuntu-latest
    needs: changes
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      - name: Restore dependencies
        run: dotnet restore Abies.Benchmarks/Abies.Benchmarks.csproj

      - name: Build benchmarks
        run: dotnet build Abies.Benchmarks/Abies.Benchmarks.csproj -c Release --no-restore

      - name: Run DOM Diffing benchmarks
        run: >
          dotnet run --project Abies.Benchmarks -c Release --no-build --
          --filter '*DomDiffingBenchmarks*'
          --exporters json
          --artifacts ./benchmark-results/diffing

      - name: Run Rendering benchmarks
        run: >
          dotnet run --project Abies.Benchmarks -c Release --no-build --
          --filter '*RenderingBenchmarks*'
          --exporters json
          --artifacts ./benchmark-results/rendering

      - name: Run Event Handler benchmarks
        run: >
          dotnet run --project Abies.Benchmarks -c Release --no-build --
          --filter '*EventHandlerBenchmarks*'
          --exporters json
          --artifacts ./benchmark-results/handlers

      - name: Merge benchmark results
        run: python3 scripts/merge-benchmark-results.py ./benchmark-results

      # ============================================
      # HISTORICAL TRACKING (gh-pages)
      # ============================================
      - name: Check if gh-pages exists
        id: gh-pages-check
        run: |
          if git ls-remote --exit-code --heads origin gh-pages > /dev/null 2>&1; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Store throughput baseline
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: "2. Micro-Benchmarks: Throughput (BenchmarkDotNet)"
          tool: benchmarkdotnet
          output-file-path: ./benchmark-results/merged/throughput.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          save-data-file: true
          skip-fetch-gh-pages: ${{ steps.gh-pages-check.outputs.exists != 'true' }}
          alert-threshold: '105%'
          comment-on-alert: true
          fail-on-alert: false
          summary-always: true
          alert-comment-cc-users: '@MCGPPeters'

      - name: Store allocation baseline
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: "2. Micro-Benchmarks: Allocations (BenchmarkDotNet)"
          tool: customSmallerIsBetter
          output-file-path: ./benchmark-results/merged/allocations.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          save-data-file: true
          skip-fetch-gh-pages: false
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          summary-always: true
          alert-comment-cc-users: '@MCGPPeters'

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: micro-benchmark-results
          path: ./benchmark-results/
          retention-days: 30

  # ============================================================================
  # E2E BENCHMARKS (js-framework-benchmark) - Source of Truth
  # ============================================================================
  # This job measures REAL user-perceived performance including:
  # - CPU benchmarks (01-09): Rendering latency (EventDispatch → Paint)
  # - Memory benchmarks (21-25): Heap usage in MB
  #
  # What it captures that micro-benchmarks miss:
  # - JS interop overhead (the biggest cost in WASM)
  # - Browser rendering pipeline (style → layout → paint)
  # - GC pressure at scale
  # - Real memory usage patterns
  #
  # Auto-triggers on:
  # 1. Manual: workflow_dispatch
  # 2. PR: Title starts with 'perf:' or 'perf('
  # 3. PR: Has 'performance' label
  # 4. PR: Modifies benchmark workflow or scripts
  # 5. Push to main: Baseline tracking
  # ============================================================================
  e2e-benchmark:
    name: E2E Benchmark (js-framework-benchmark)
    runs-on: ubuntu-latest
    needs: changes
    if: >
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      needs.changes.outputs.perf-pr == 'true' ||
      needs.changes.outputs.e2e-scripts == 'true' ||
      contains(github.event.pull_request.labels.*.name, 'performance')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Clone js-framework-benchmark
        run: |
          git clone --depth 1 https://github.com/krausest/js-framework-benchmark.git ../js-framework-benchmark-fork

      # Set up Abies framework in the benchmark repo
      - name: Setup Abies framework
        run: |
          # Create framework directory structure
          mkdir -p ../js-framework-benchmark-fork/frameworks/keyed/abies/src
          mkdir -p ../js-framework-benchmark-fork/frameworks/keyed/abies/bundled-dist

          # Copy framework files from contrib (package.json AND package-lock.json required by /ls endpoint)
          cp $GITHUB_WORKSPACE/contrib/js-framework-benchmark/package.json ../js-framework-benchmark-fork/frameworks/keyed/abies/
          cp $GITHUB_WORKSPACE/contrib/js-framework-benchmark/package-lock.json ../js-framework-benchmark-fork/frameworks/keyed/abies/
          cp -r $GITHUB_WORKSPACE/contrib/js-framework-benchmark/src/* ../js-framework-benchmark-fork/frameworks/keyed/abies/src/

          # Copy Abies library source (excluding bin/obj to avoid stale artifacts)
          mkdir -p ../js-framework-benchmark-fork/frameworks/keyed/abies/src/Abies
          rsync -a --exclude='bin' --exclude='obj' $GITHUB_WORKSPACE/Abies/ ../js-framework-benchmark-fork/frameworks/keyed/abies/src/Abies/

          # Copy Global folder (referenced by Directory.Build.props)
          cp -r $GITHUB_WORKSPACE/Global ../js-framework-benchmark-fork/frameworks/keyed/abies/src/Global

          # Copy build configuration
          cp $GITHUB_WORKSPACE/Directory.Build.props ../js-framework-benchmark-fork/frameworks/keyed/abies/src/
          cp $GITHUB_WORKSPACE/global.json ../js-framework-benchmark-fork/frameworks/keyed/abies/src/

      # Cache npm dependencies (key based on static version since files are outside workspace)
      - name: Cache npm dependencies
        uses: actions/cache@v4
        with:
          path: ~/.npm
          key: npm-js-framework-benchmark-${{ runner.os }}-v1
          restore-keys: |
            npm-js-framework-benchmark-${{ runner.os }}-

      # Cache Chrome browser for Selenium
      - name: Cache Chrome for Testing
        uses: actions/cache@v4
        with:
          path: ~/.cache/selenium
          key: selenium-chrome-${{ runner.os }}-v1
          restore-keys: |
            selenium-chrome-${{ runner.os }}-

      - name: Build Abies for benchmark
        run: |
          cd ../js-framework-benchmark-fork/frameworks/keyed/abies/src
          # Publish
          dotnet publish -c Release
          # Copy to bundled-dist
          cp -r bin/Release/net10.0/publish/wwwroot/* ../bundled-dist/

      - name: Install benchmark dependencies
        run: |
          cd ../js-framework-benchmark-fork
          npm ci
          cd webdriver-ts
          npm ci
          # Compile TypeScript to dist/ folder
          npm run compile

      - name: Start benchmark server
        run: |
          cd ../js-framework-benchmark-fork
          npm start &
          # Wait for server to be ready (up to 30 seconds)
          echo "Waiting for benchmark server to start..."
          for i in {1..30}; do
            if curl -s http://localhost:8080 > /dev/null 2>&1; then
              echo "Server is ready!"
              break
            fi
            if [ $i -eq 30 ]; then
              echo "Error: Server failed to start within 30 seconds"
              exit 1
            fi
            sleep 1
          done

      - name: Run E2E benchmarks (all CPU benchmarks)
        run: |
          cd ../js-framework-benchmark-fork/webdriver-ts
          # Run ALL js-framework-benchmark CPU benchmarks
          # Format: keyed/frameworkname per README
          #
          # CPU Benchmarks (01-09):
          # - 01_run1k: Create 1,000 rows
          # - 02_replace1k: Replace all 1,000 rows (5 warmup iterations)
          # - 03_update10th1k: Update every 10th row for 10k table (5 warmup)
          # - 04_select1k: Select/highlight a row (5 warmup)
          # - 05_swap1k: Swap 2 rows (5 warmup)
          # - 06_remove-one-1k: Remove one row (5 warmup)
          # - 07_create10k: Create 10,000 rows
          # - 08_create1k-after1k_x2: Append 1,000 rows to 1k table
          # - 09_clear1k: Clear table with 1k rows
          #
          npm run bench -- --headless --framework keyed/abies --benchmark 01_run1k
          npm run bench -- --headless --framework keyed/abies --benchmark 02_replace1k
          npm run bench -- --headless --framework keyed/abies --benchmark 03_update10th1k
          npm run bench -- --headless --framework keyed/abies --benchmark 04_select1k
          npm run bench -- --headless --framework keyed/abies --benchmark 05_swap1k
          npm run bench -- --headless --framework keyed/abies --benchmark 06_remove-one-1k
          npm run bench -- --headless --framework keyed/abies --benchmark 07_create10k
          npm run bench -- --headless --framework keyed/abies --benchmark 08_create1k-after1k_x2
          npm run bench -- --headless --framework keyed/abies --benchmark 09_clear1k

      - name: Run Memory benchmarks
        run: |
          cd ../js-framework-benchmark-fork/webdriver-ts
          # Memory benchmarks measure heap usage in MB
          # 21_ready-memory: Ready memory (memory after page load)
          # 22_run-memory: Run memory (after creating 1k rows)
          # 25_clear-memory: Clear memory (after 5 create/clear cycles)
          npm run bench -- --headless --framework keyed/abies --benchmark 21_ready-memory
          npm run bench -- --headless --framework keyed/abies --benchmark 22_run-memory
          npm run bench -- --headless --framework keyed/abies --benchmark 25_clear-memory

      - name: Fetch baseline from gh-pages (if not in repo)
        if: ${{ !hashFiles('benchmark-results/baseline.json') }}
        continue-on-error: true
        run: |
          # Try to fetch baseline from gh-pages branch
          mkdir -p benchmark-results
          if git show origin/gh-pages:data/e2e-baseline.json > benchmark-results/baseline.json 2>/dev/null; then
            echo "✅ Fetched baseline from gh-pages"
          else
            echo "⚠️ No baseline found in gh-pages - first run will create one"
          fi

      - name: Compare against baseline
        run: |
          python3 scripts/compare-benchmark.py \
            --results-dir ../js-framework-benchmark-fork/webdriver-ts/results \
            --baseline benchmark-results/baseline.json \
            --threshold 5.0 \
            --framework abies

      - name: Convert E2E results to benchmark format
        run: |
          python3 scripts/convert-e2e-results.py \
            --results-dir ../js-framework-benchmark-fork/webdriver-ts/results \
            --output ./e2e-benchmark-results.json \
            --output-memory ./e2e-benchmark-memory.json \
            --framework abies

      - name: Check if gh-pages exists
        id: gh-pages-check
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          if git ls-remote --exit-code --heads origin gh-pages > /dev/null 2>&1; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Store E2E benchmark trends to gh-pages (main only)
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.gh-pages-check.outputs.exists == 'true'
        with:
          name: "1. E2E Benchmark (js-framework-benchmark)"
          tool: customSmallerIsBetter
          output-file-path: ./e2e-benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          save-data-file: true
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          summary-always: true
          alert-comment-cc-users: '@MCGPPeters'

      - name: Store E2E memory benchmark trends to gh-pages (main only)
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.gh-pages-check.outputs.exists == 'true'
        with:
          name: "1. E2E Benchmark: Memory (js-framework-benchmark)"
          tool: customSmallerIsBetter
          output-file-path: ./e2e-benchmark-memory.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          save-data-file: true
          alert-threshold: '120%'
          comment-on-alert: true
          fail-on-alert: false
          summary-always: true
          alert-comment-cc-users: '@MCGPPeters'

      - name: Copy results to workspace for upload
        if: always()
        run: |
          mkdir -p ./e2e-results
          cp -r ../js-framework-benchmark-fork/webdriver-ts/results/* ./e2e-results/ 2>/dev/null || true
          cp ./e2e-benchmark-results.json ./e2e-results/ 2>/dev/null || true
          cp ./e2e-benchmark-memory.json ./e2e-results/ 2>/dev/null || true

      - name: Upload E2E benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-benchmark-results
          path: ./e2e-results/
          retention-days: 30
